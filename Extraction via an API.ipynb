{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01e40fed-898f-47e2-a32c-97403106869d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple API Data Extraction Example\n",
    "65,000 records in chunks of 100\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "#def extract_api_data_simple(base_url, headers=None, total_records=65000, chunk_size=100):\n",
    "def extract_api_data_simple( total_records=65000, chunk_size=100):\n",
    "\n",
    "    \"\"\"\n",
    "    Simple function to extract API data in chunks\n",
    "    \n",
    "    Args:\n",
    "        base_url: API endpoint URL\n",
    "        headers: HTTP headers (dict)\n",
    "        total_records: Total number of records to extract\n",
    "        chunk_size: Records per API call\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: All extracted data\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    total_chunks = math.ceil(total_records / chunk_size)\n",
    "    \n",
    "    print(f\"üöÄ Extracting {total_records} records in {total_chunks} chunks of {chunk_size}\")\n",
    "    \n",
    "    for chunk in range(total_chunks):\n",
    "        \n",
    "        # Calculate offset and limit\n",
    "        offset = chunk * chunk_size\n",
    "        print('offset', offset)\n",
    "        print('chunk', chunk)\n",
    "        print('chunk_size', chunk_size)\n",
    "        limit = min(chunk_size, total_records - offset)\n",
    "        print('limit', limit)\n",
    "        \n",
    "        try:\n",
    "            # API request parameters\n",
    "            params = {\n",
    "                'offset': offset,\n",
    "                'limit': limit,\n",
    "                # Add other parameters as needed:\n",
    "                # 'page': chunk + 1,\n",
    "                # 'per_page': chunk_size,\n",
    "            }\n",
    "            \n",
    "            # Make API call\n",
    "            print(f\"üì• Fetching chunk {chunk + 1}/{total_chunks} (records {offset}-{offset + limit - 1})\")\n",
    "            \n",
    "            #response = requests.get(base_url, params=params, headers=headers, timeout=30)\n",
    "            #response.raise_for_status()\n",
    "            \n",
    "            # Parse response\n",
    "            #data = response.json()\n",
    "            \n",
    "            # Extract records (adjust based on your API response structure)\n",
    "            if isinstance(data, list):\n",
    "                records = data\n",
    "            elif 'data' in data:\n",
    "                records = data['data']\n",
    "            elif 'results' in data:\n",
    "                records = data['results']\n",
    "            else:\n",
    "                records = data.get('items', [])\n",
    "            \n",
    "            if records:\n",
    "                # Convert to DataFrame and process\n",
    "                chunk_df = pd.DataFrame(records)\n",
    "                \n",
    "                # Add metadata\n",
    "                chunk_df['chunk_number'] = chunk\n",
    "                chunk_df['extracted_at'] = datetime.now()\n",
    "                \n",
    "                all_data.append(chunk_df)\n",
    "                print(f\"‚úÖ Processed {len(records)} records\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No records in chunk {chunk + 1}\")\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.1)  # 100ms delay between requests\n",
    "            \n",
    "            # Progress update\n",
    "            if (chunk + 1) % 50 == 0:\n",
    "                progress = ((chunk + 1) / total_chunks) * 100\n",
    "                print(f\"üìä Progress: {progress:.1f}%\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in chunk {chunk + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all chunks\n",
    "    if all_data:\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"üéâ Extraction complete! Total records: {len(final_df)}\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"üí• No data extracted!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Example 1: Your configuration\n",
    "    print(\"üìä API Data Extraction - 65,000 Records\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # CUSTOMIZE THESE VALUES FOR YOUR API:\n",
    "    API_URL = \"https://your-api-endpoint.com/data\"\n",
    "    API_HEADERS = {\n",
    "        'Authorization': 'Bearer your-token-here',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Extract data\n",
    "    # df = extract_api_data_simple(\n",
    "    #     base_url=API_URL,\n",
    "    #     headers=API_HEADERS,\n",
    "    #     total_records=65000,\n",
    "    #     chunk_size=100\n",
    "    # )\n",
    "    \n",
    "    # Save to file\n",
    "    # if not df.empty:\n",
    "    #     filename = f\"api_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    #     df.to_csv(filename, index=False)\n",
    "    #     print(f\"üíæ Data saved to: {filename}\")\n",
    "    \n",
    "    # Example 2: Test with public API\n",
    "    print(\"\\nüß™ Testing with JSONPlaceholder API (100 records)\")\n",
    "    test_df = extract_api_data_simple(\n",
    "        base_url=\"https://jsonplaceholder.typicode.com/posts\",\n",
    "        total_records=100,\n",
    "        chunk_size=10\n",
    "    )\n",
    "    \n",
    "    if not test_df.empty:\n",
    "        print(f\"\\nüìã Test Results:\")\n",
    "        print(f\"Records: {len(test_df)}\")\n",
    "        print(f\"Columns: {list(test_df.columns)}\")\n",
    "        print(f\"\\nFirst 3 records:\")\n",
    "        print(test_df.head(3))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Extraction via an API",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
